---
title: "Cyclistic Rider Behavior Cleaning Report"
author: "JohnPaul Medina"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=FALSE}
source(here::here("simulation/simConfig.R"))
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

Data cleaning is a critical step in any data analysis project. The accuracy and 
reliability of insights depend on the quality of the underlying data.  

This simulation of the full process is done in a parallel process using a 
simulation folder to show the cleaning process described in this document 
without overwriting the current version of the project's cleaned dataset.

---


> **Project Documentation**:  
> View the full **cleaning, EDA, and changelog logs**  
> [Click here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRsdTcZUKUd6BXzZpSvwYAP8hJBCRDVilBmd9sOeeCMLLNRvnmaT5X8OIv_txawY_CcYy0frfpHOpTK/pubhtml)


---


## Part 1: Initial Dataset Exploration and Verification

* This script is written as introductory exploration of datasets for familiarity 
and understanding.
* After initial exploration, the script checks if every column in every dataset 
contains the same column name. 
* Finally, the script checks if all of the corresponding columns of every
dataset is of the same data type.


### Initial view of first dataset

To familiarize ourselves with the dataset structure, we first load a single file 
to examine its column names and data types.

```{r initial-dataset-exploration}
# Load the first dataset for initial survey of column characteristics
jan_2024 <- read.csv(file.path(sim_raw_data_dir,"202401-divvy-tripdata.csv"))

# Display the column names and their respective data types. 
str(jan_2024)
```


### Check Column Name Consistency Across Datasets

Each dataset spans 13 months (Jan 2024 - Jan 2025). Before merging, we must 
verify that all datasets contain identical column names to prevent mismatches.

```{r column-name-consistency-check}
check_column_names <- function(directory){
  # Compile all the raw dataset into a list
  raw_files <- list.files(path=sim_raw_data_dir, pattern = "*.csv",
                          full.names = TRUE)
  # Read the column names for each file
  column_names_list <- lapply(raw_files, 
                              function(file) colnames(read.csv(file, nrows=1)))
  
  # Check if all datasets have the same column names
  # Unique function removes all duplicates except for the first one.
  unique_columns <- unique(column_names_list)
  
  # If there is only one left in the list, then all column names match
  if(length(unique_columns)==1){
    print("All datasets have consistent column names.")
  }else{
    print("Column name mismatch found!")
    # Print each unique column set separately for clarity
    for(i in seq_along(unique_columns)){
      cat("\nUnique Column Set",i,":\n")
      print(unique_columns[[i]])
    }
  }
}
# Call the function
check_column_names(sim_raw_data_dir)

```


### Check Consistent Column Data Type

Ensuring each column retains the same data type across all datasets prevents 
compatibility issues when merging.

```{r column-data-type-check}
# This function checks if all the data types of corresponding columns in each
# of the datasets are the same.
check_data_types <- function(directory){
  # List of all csv files
  files <- list.files(path=directory, pattern = "*.csv", full.names=TRUE)
  
  # Read and check column data types for each file
  data_types_list <- lapply(files, function(file){
    df <- read.csv(file, na.strings = c("","NA")) # Treat empty strings as NA
    sapply(df,class) # Get the data types
  })
  
  # Check if all the datasets have consistent data types
  unique_data_types <- unique(data_types_list)
  
  if (length(unique_data_types) == 1) {
    print("All datasets have consistent data types.")
  } else {
    print("Data type mismatch detected! Showing unique data types:")
    print(unique_data_types)
  }
}
# Run the function
check_data_types(sim_raw_data_dir)
```


---  


## Part 2: Merging the Data and Setting Missing Values to NA

This part merges the 13 monthly datasets into one combined dataset and setting 
the missing values to NA for easier identification and to avoid errors in 
computations during the analysis process.


### Merging the Data and Setting Missing Values to NA

After we have verified that the column names are consistent and that their 
corresponding data types are matching, now we can merge the datasets into one.

```{r merging-the-datasets-and-set-missing-values-to-NA}
# Compile all csv file names  from the sim_raw_data_dir
sim_raw_files <- list.files(path=sim_raw_data_dir,pattern="*.csv", 
                            full.names=TRUE)

# Read all CSV files, replace empty/missing data with NA, and combine into one dataset
sim_combined_dataset <- bind_rows(lapply(sim_raw_files, function(file) {
  read.csv(file, na.strings = c("", "NA"))  # Convert "" and "NA" to NA
}))

#Verify that the beginning of the combined dataset is Jan 2024
head(sim_combined_dataset)
# Verify that the tail of the combined dataset is Jan 2025
tail(sim_combined_dataset)

# Save an RDS version of the dataset for optimal loading and access
saveRDS(sim_combined_dataset, file.path(sim_processed_data_dir, "sim_combined_raw_data.rds"))
# Save a csv version of the dataset for sharing
write.csv(sim_combined_dataset, 
          file.path(sim_processed_data_dir, "sim_combined_raw_data.csv"),row.names = FALSE)
```


---

## Part 3: Converting `started_at` and `ended_at` Character Dates into POSIXct

Based on our exploration and analysis, the `started_at` and `ended_at` columns 
of the dataset are in `character` format. We need to convert them into `POSIXct` 
format to facilitate computation of **time** specific values. 

```{r convert-character-dates-to-POSIXct}
# Load the combined raw data
sim_raw_combined_data <- 
  readRDS(file.path(sim_processed_data_dir, "sim_combined_raw_data.rds"))

# Convert character dates in started_at and ended_at into POSIXct
sim_convert_dates <- function(df){
  df$started_at <- ymd_hms(df$started_at, tz = timezone)
  df$ended_at <- ymd_hms(df$ended_at, tz = timezone)
  return(df)
}

# Apply the function
sim_processed_data <- sim_convert_dates(sim_raw_combined_data)

# Check that the conversion was successful
message("Date conversion completed.")
print("started_at column preview")
str(sim_processed_data$started_at)
print("ended_at column preview")
str(sim_processed_data$ended_at)

# Save processed data as RDS, given specific name for this simulation
saveRDS(sim_processed_data, 
        file.path(sim_processed_data_dir,"processed_combined_data_posixct.rds"))

message("RDS file saved successfully: processed_combined_data_posixct.rds")

```

---

## Part 4: Check and Count Missing Values per Column

Checking for missing values in the dataset by column. Missing values can skew 
the result of data use and interpretation; thus, understanding what data are 
missing and making a decision on how to handle them is important.

### Initial Analysis and Plan

The highest number of missing values is **1,128,726** which represents about 
20% of the entire datase. Further investigation is needed to determine how this 
affects the project's objectives and how this may be handled.

```{r checking-for-missing-data-by-column}
# Load the most recent cleaned rds
df <- 
  readRDS(file.path(sim_processed_data_dir, 
                    "processed_combined_data_posixct.rds"))

# Function that counts the missing values per column
# There is a more direct and easier way to do this but this creates a table
# that displays the information in a more accessible format.
sim_missing_values_summary <- function(df){
  sim_missing_counts <- sapply(df, function(x) sum(is.na(x)))
  
  # Convert the summary into a data frame
  sim_missing_summary <- data.frame(Column=names(sim_missing_counts),
                                    Missing_Count=sim_missing_counts)
  
  # Sort by highest missing values first
  sim_missing_summary <- sim_missing_summary[order(-sim_missing_summary$Missing_Count),]
  
  return(sim_missing_summary)
}

sim_missing_summary <- missing_values_summary(df)

# Display the missing values summary as a formatted table
knitr::kable(sim_missing_summary, caption = "Missing Values Summary by Column")

```
```{r save-missing-values-summary, echo=FALSE, include=FALSE}
# Save the summary for use in report and documentation.
write.csv(sim_missing_summary, file.path(sim_processed_data_dir,
    "sim_missing_values_summary.csv"), row.names = FALSE)
message("Missing values summary saved in processed folder.")

```


---

## Step 5: Trim Whitespace and Ensure `member_casual` Data are Lower Case

Trimming whitespaces from character columns will ensure that each data are 
unique in such a way that extra spaces do not create multiple instance of the 
same value.  

Ensuring that the `member_casual` column data are all in lower case prevents 
data processing error because of accidental case discrepancy.

```{r trim-whitespace-and-lower-case}
# Load most recent simulation cleaned rds
sim_cleaned_data <- 
  readRDS(file.path(sim_processed_data_dir, 
                    "processed_combined_data_posixct.rds"))

# List character columns
char_cols <- c("ride_id", "rideable_type", "start_station_name", 
               "start_station_id", "end_station_name", "end_station_id", 
               "member_casual")

# Trim white spaces among character columns
sim_cleaned_data[char_cols] <- lapply(sim_cleaned_data[char_cols], trimws)

# Ensure that member_casual column data are all in lower case
sim_cleaned_data$member_casual <- tolower(sim_cleaned_data$member_casual)

message("Character columns whitespaces removed, casual_member column standardized.")

# Save processed data as RDS, given specific name for this simulation
saveRDS(sim_cleaned_data, 
        file.path(sim_processed_data_dir,"trimmed_sim_cleaned_data.rds"))

message("RDS file saved successfully: trimmed_sim_cleaned_data.rds")

```

---

## Step 6: Compute and Create the `ride_duration` Column and Flag Invalid Rides

One of the main focus of this analysis is determining the differences between 
**casual and member riding patterns**. Creating the `ride_duration` column is 
essential as it will be used throughout the exploratory data analysis (EDA) 
process. 

```{r compute-ride-duration-and-flag-invalid-duration}
# Load the most current simulation data
sim_cleaned_data <- 
  readRDS(file.path(sim_processed_data_dir,"trimmed_sim_cleaned_data.rds"))

# Function that calculates the ride duration in terms of minutes
calculate_ride_duration <- function(df){
  df$ride_duration <- as.numeric(difftime(df$ended_at, df$started_at, 
                                          units="mins"))
  
  # Flag invalid data (zero and negative)
  df$invalid_duration <- df$ride_duration <= 0
  
  # Count the number of flagged invalid rows
  num_invalid <- sum(df$invalid_duration)
  
  cat("Found", num_invalid, "rides with invalid durations(<=0 minutes.\n")
  cat("invalid rides flagged but not removed. \n")
  
  return(df)
}

# Call the function
sim_cleaned_dataset <- calculate_ride_duration(sim_cleaned_data)

# Save processed data as RDS, given specific name for this simulation
saveRDS(sim_cleaned_dataset, 
        file.path(sim_processed_data_dir,"sim_ride_duration_cleaned_data.rds"))

message("RDS file saved successfully: sim_ride_duration_cleaned_data.rds")
```
### `ride_duration` Initial Note

`ride_duration` contains negative and zero values, further investigation is 
needed to check suspicious large values and data integrity of rides that are 
zero to one minute. Exploratory analysis of ride_duration is necessary to 
determine if there are data that should be excluded from the analysis moving 
forward.

